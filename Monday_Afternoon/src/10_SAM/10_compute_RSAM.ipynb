{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc74389a-0ad7-40f0-a256-f7c3260939cc",
   "metadata": {},
   "source": [
    "# RSAM notebook\n",
    "\n",
    "In this tutorial, we will explore Real-time Seismic Amplitude Measurement (RSAM) data. RSAM is, by definition, computed on raw seismic data, so we can also think of it as \"Raw\" Seismic Amplitude Measurement, to distinguish from similar measurements we will make later on velocity and displacement seismograms.\n",
    "\n",
    "## 1. Background\n",
    "\n",
    "### 1.1 Motivation\n",
    "\n",
    "Imagine it is Spring 1985, and you are at the only Seismologist at the USGS Cascades Volcano Observatory (CVO). Tremor is appearing on the helical drum recorders, and has appeared before most of the explosive eruptions over the past 5 years. The authorities want to know if the tremor now is as strong as it was right before the catastrophic May 18, 1980 sector collapse. \n",
    "\n",
    "Volcano-seismic monitoring was simple, and largely consisted of:\n",
    "1. Counting the number of earthquakes each day on the drum records (\"daily counts\")\n",
    "2. Locating and mapping volcano-tectonic earthquakes, and estimating their magnitudes (\"catalog production/analysis\")\n",
    "3. During heightened times of unrest, manning an Operations Room 24-7 with analysts continuously watching the drums, and communicating with field crews by 2-way radio (\"real-time monitoring\")\n",
    "\n",
    "So all you have is the drum records (hundreds of large sheets of paper) and the catalog. You don't have any digital version of the continuous seismic data sitting on a hard drive, or on a CD. Why? \n",
    "- CD-ROM drives didn't appear until ~1990\n",
    "- hard drive storage was too expensive. Here is a quick calculation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24904be6-8a21-4121-8c04-fadd3655d4ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Algorithm to compute raw storage space needed for seismic data\n",
    "def storage_space(samplingRate=100, bitsPerSample=32, numComponentsPerStation=3, numStations=10):\n",
    "    BITS_PER_BYTE = 8\n",
    "    SECONDS_PER_DAY = 60 * 60 * 24\n",
    "    bytesPerGb = 1024**3\n",
    "    gbPerDayPerChannel = (samplingRate * (bitsPerSample/BITS_PER_BYTE) * SECONDS_PER_DAY) / bytesPerGb\n",
    "    gbPerDayNetwork = gbPerDayPerChannel * numComponentsPerStation * numStations\n",
    "    print(f\"Raw data requires {gbPerDayNetwork:.02f} GB of storage per day, and {gbPerDayNetwork * 365:.0f} GB per year\")\n",
    "\n",
    "    dollarsPerTB = {'1985':31400000, '2000':4070, '2023':14.3}\n",
    "    print(\"\\nStorage cost for 1 year of data, in different years:\")\n",
    "    for key in dollarsPerTB:  \n",
    "        print(f\"{key}: US${(gbPerDayNetwork * 365 * dollarsPerTB[key]/1024):,.0f}\")\n",
    "\n",
    "    print(\"Data from https://ourworldindata.org/grapher/historical-cost-of-computer-memory-and-storage\")\n",
    "\n",
    "\n",
    "storage_space(samplingRate=100, bitsPerSample=32, numComponentsPerStation=3, numStations=10)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "489305ac-af8b-4606-8c19-acec9c5c049f",
   "metadata": {},
   "source": [
    "So back in 1985, hard drive storage for just one year of data from the Mount St. Helens seismic network would have cost ~US$10 Million!\n",
    "Given these costs, STA/LTA algorithms were used to capture anomalous signals - volcanic earthquakes - while the continuous data were generally discarded (or at best, recorded to tape).\n",
    "\n",
    "Anyway, so you don't have an easy way to compare tremor levels. But you sure as hell aren't going to be caught in this situation again! So what can you do? <em><font color='green'>You can store a massively downsampled version of the continuous seismic data instead!</font></em>\n",
    "\n",
    "This idea led to the Real-time Seismic Amplitude Measurement (RSAM) system."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c69adc2-c75b-47c9-bd29-e9013f82dc53",
   "metadata": {},
   "source": [
    "### 1.2 Original RSAM system\n",
    "\n",
    "The RSAM system was built around a 8-bit analog-to-digital-converter PC card: sofware was too slow in those days. Components of the original RSAM system were:\n",
    "\n",
    "<font color='blue'>\n",
    "<ol>\n",
    "<li>Real-time bar graphs: showing average seismic amplitudes over last 2.56 s, 1 minute, and 10 minutes</li>\n",
    "<li><b>1 minute and 10 minute mean signal amplitudes, logged to binary files. This is what most volcano-seismologists today think of as \"RSAM data\"!</b></li>\n",
    "<li>\"RSAM events\": created by a simple STA/LTA detector running on each channel (NSLC)</li>\n",
    "<li>Multi-station event (e.g. earthquake) and tremor alarm systems</li>\n",
    "<li>Trends in RSAM data and other datasets (e.g. earthquake counts, tiltmeter data, gas flux, deformation, etc.) could be visualized with another software package called \"BOB\"</li>\n",
    "</ol></font>\n",
    "\n",
    "<table border=1><tr><td><img width=100% src=\"images/EndoMurray1991fig7.png\" ></td><td>Fig 7 from Endo & Murray (1991). Top panel shows RSAM event rate at closest station to Pinatubo. Bottom 3 panels show RSAM data from stations at increasing distances. 30 days of data are show</td></table></tr></table>\n",
    "\n",
    "In the figure above, 30 days of RSAM data are shown for three seismic stations. Loading and plotting 30 days of raw seismic data takes a while, but 1-minute RSAM data downsamples the raw seismic data by a factor of 6,000 (assuming a 100 Hz sampling rate), so long RSAM timeseries (hours, days, weeks, months, etc.) can be quickly loaded and plotted.\n",
    "\n",
    "Reference:\n",
    "- Endo, E.T., Murray, T. Real-time Seismic Amplitude Measurement (RSAM): a volcano monitoring and prediction tool. Bull Volcanol 53, 533â€“545 (1991).__[https://doi.org/10.1007/BF00298154](pdf/RSAM_EndoMurray1991.pdf)__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e92e6070-de05-4007-b835-5fdeb75bf331",
   "metadata": {},
   "source": [
    "## 2. Computing RSAM data\n",
    "\n",
    "### 2.1 Simple example\n",
    "\n",
    "Here is a minimal example of computing RSAM data from an ObsPy Stream object. The data come from station REF at Redoubt Volcano in Alaska on 2009/03/22."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "566f9e92-e143-444f-9bc5-f3647120bd25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import obspy\n",
    "sys.path.append('../lib')\n",
    "from SAM import RSAM\n",
    "st = obspy.read(os.path.join('..','..','data','continuous','SDS','2009','AV','REF','EHZ.D', 'AV.REF..EHZ.D.2009.081' ))\n",
    "st.plot();\n",
    "rsamObj = RSAM(stream=st, sampling_interval=600)\n",
    "rsamObj.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82097abe-c61c-4f64-94f5-7241a8d2a60c",
   "metadata": {},
   "outputs": [],
   "source": [
    "help(rsamObj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f152bc01-7016-4024-bb95-2c678d36b5d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(rsamObj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "993574c7-1e2c-4648-b19d-df5f79a56fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('RSAM dataframe for one Trace id (net.sta.loc.chan):')\n",
    "print(rsamObj.dataframes['MV.MLGT..EHZ'].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16663dd0-f42e-493d-acdf-f39a64f67c27",
   "metadata": {},
   "source": [
    "There are just two columns, which are 'time', and 'mean'. \n",
    "- 'time' is in Unix epoch seconds (since 1970-01-01 00:00:00)\n",
    "- 'mean' just holds the mean seismic amplitude within that 60-s time window (Sampling Interval=60.0s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c9aee4b-54f9-466b-acc0-3d06e1ac34f3",
   "metadata": {},
   "source": [
    "## 2.2 Non-trivial example\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cafbb186-1fe8-4c15-8d0d-51cca704a672",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Header\n",
    "import os\n",
    "import sys\n",
    "import obspy\n",
    "from obspy.clients.filesystem.sds import Client as sdsclient\n",
    "sys.path.append('../lib')\n",
    "import setup_paths\n",
    "paths = setup_paths.paths\n",
    "from SAM import RSAM\n",
    "\n",
    "# Compute RSAM in 1-day chunks for multiple network-station-location-channel's\n",
    "mySDSclient = sdsclient(paths['SDS_DIR'])\n",
    "startTime = obspy.core.UTCDateTime(2001,2,27)\n",
    "endTime = obspy.core.UTCDateTime(2001,3,3)\n",
    "secondsPerDay = 60 * 60 * 24\n",
    "numDays = (endTime-startTime)/secondsPerDay\n",
    "daytime = startTime\n",
    "while daytime < endTime:\n",
    "    print(f'Loading Stream data for {daytime}')\n",
    "    st = mySDSclient.get_waveforms(\"MV\", \"*\", \"*\", \"[SBEHCD]*\", daytime, daytime+secondsPerDay)\n",
    "    print(f'- got {len(st)} Trace ids')\n",
    "    print(f'Computing RSAM metrics for {daytime}, and saving to pickle files')\n",
    "    rsamMV24h = RSAM(stream=st, sampling_interval=60)\n",
    "    #rsamMV24h.write(paths['SAM_DIR'])\n",
    "    daytime += secondsPerDay\n",
    "del mySDSclient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0179f63-63cd-4261-93ae-15932d093f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read all the RSAM data back, and plot\n",
    "rsamObj = RSAM.read(startTime, endTime, SAM_DIR=paths['SAM_DIR'])\n",
    "print(rsamObj)\n",
    "rsamObj.plot(metrics='median')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15fcf65e-51bb-4784-a085-897c8565f386",
   "metadata": {},
   "source": [
    "## 3. RSAM data processing\n",
    "\n",
    "We can process the data in various ways, e.g. using select(), downsample(), "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19ee2ae6-354d-469f-b2f0-2d4a02d93550",
   "metadata": {},
   "source": [
    "## 3. Legacy RSAM data \n",
    "\n",
    "### 3.1 Loading legacy RSAM data from binary files\n",
    "\n",
    "The RSAM system was used at many observatories, and so many observatories have archives of RSAM binary files. But we can read these, making them Interoperable and Reusable. (Tiltmeter was saved in the same format, and so can also be read).\n",
    "\n",
    "Next we will load 1 year of RSAM data for 8 stations recorded by the original RSAM system that was deployed in Montserrat. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38f21b10-8a61-4401-81c7-299c5a861fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import glob\n",
    "import obspy\n",
    "sys.path.append('../lib')\n",
    "import setup_paths\n",
    "paths = setup_paths.paths\n",
    "from SAM import RSAM\n",
    "\n",
    "stime = obspy.core.UTCDateTime(1997,1,1,0,0,0)\n",
    "etime = obspy.core.UTCDateTime(1997,12,31,23,59,59)\n",
    "files = glob.glob(os.path.join(paths['SAMBINARY_DIR'], f'M???{stime.year}.DAT'))\n",
    "stations = list(set([os.path.basename(file)[0:4] for file in files]))\n",
    "rsamObj = RSAM.readRSAMbinary(paths['SAMBINARY_DIR'], stations, stime, etime)\n",
    "print(rsamObj)\n",
    "rsamObj.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b07a9ab-e778-4fa5-85e5-1fd7ee2d89db",
   "metadata": {},
   "source": [
    "### 3.2 Converting legacy RSAM binary files to modern RSAM CSV/Pickle files\n",
    "Since we have already read the binary files into a (single) RSAM object, writing them to modern RSAM data format is as simple as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b509f74-40e6-4a8a-a57c-b4ca40b4dd47",
   "metadata": {},
   "outputs": [],
   "source": [
    "rsamObj.write(paths['SAM_DIR'], ext='csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a25a212-9ce5-4508-979e-f0db02379f74",
   "metadata": {},
   "source": [
    "## 4. RSAM data processing and analysis\n",
    "\n",
    "### 4.1 read and plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcdf574f-b80a-4f32-8667-26b8ab4e316d",
   "metadata": {},
   "source": [
    "Next we will:\n",
    "- (re-)read (from disk) the RSAM data from 1996-02-15 to 1996-10-12 for select SEED ids\n",
    "- plot the data. By default, the plot() method will convert RSAM dataframes into an ObsPy Stream object, so it can be plotted in a familiar way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fa2dad1-4a04-4846-b131-1036185946ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "startt = obspy.core.UTCDateTime(1996,2,15)\n",
    "endt = obspy.core.UTCDateTime(1996,10,12)\n",
    "rsamObj = RSAM.read(startt, endt, trace_ids=['MV.MLGT..EHZ', 'MV.MGAT..EHZ', 'MV.MRYT..EHZ', 'MV.MGHZ..EHZ'], SAM_DIR=paths['SAM_DIR'], ext='csv')\n",
    "rsamObj.plot()   \n",
    "print(rsamObj)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2dc366f-8c1d-4c9d-bca6-1ed03624753c",
   "metadata": {},
   "source": [
    "These RSAM plots above show the following general features:\n",
    "1. Low seismicity in February and March.\n",
    "2. An increase in seismicity around April 1st persists throughout to June. This period included the first pyroclastic density current (PDC) that reached the ocean on May 12, 1996.\n",
    "3. A more significant increase in activity about 2/3rds of the way through July 1996. This was a time period in which the seismicity and the lava dome extrusion rate significantly increased, leading to numerous PDCs that reached the ocean, and even travelled for some distance upon the water. The increase is particularly noticeable on MV.MLGT..EHZ (3rd trace) as this was close to the Tar River Valley, where most PDCs were directed.\n",
    "4. A sharp drop in seismicity from September 18, 1996, onwards.\n",
    "\n",
    "These features may be more obvious if we smooth the data, which we can do with the downsample() method:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "976de361-926a-4fe8-b04b-bfd5d22d19e6",
   "metadata": {},
   "source": [
    "### 4.2 Trim and Downsample "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1d969a7-66d4-4038-abdd-267979ff4175",
   "metadata": {},
   "outputs": [],
   "source": [
    "startt = obspy.core.UTCDateTime(1996,7,15)\n",
    "endt = obspy.core.UTCDateTime(1996,9,1)\n",
    "\n",
    "# trim\n",
    "rsamObj.trim(starttime=startt, endtime=endt)\n",
    "\n",
    "# downsample\n",
    "rsamObjHourly = rsamObj.downsample(new_sampling_interval=3600) \n",
    "\n",
    "# plot\n",
    "rsamObjHourly.plot()\n",
    "\n",
    "# print\n",
    "print(rsamObjHourly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bad99e1-dabf-4630-ba06-a13b11dc0473",
   "metadata": {},
   "outputs": [],
   "source": [
    "There are various periods here where there seem to be cycles in RSAM. Let us look at early August period in more detail:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ea47a28-f75f-4166-85c1-3b406b3541fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "rsamObjSummer= rsamObj.copy()\n",
    "rsamObjSummer.trim(starttime=obspy.core.UTCDateTime(1996,8,1), endtime=obspy.core.UTCDateTime(1996,8,8))\n",
    "rsamObjSummer.plot(kind='stream', equal_scale=False) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c74ec12-082a-4c9a-8159-f1c1780f641c",
   "metadata": {},
   "source": [
    "These are remarkable cycles in RSAM. They appear to be about 4-6 hours apart. This is a phenomenon called \"banded tremor\". During these tremor bands, visual observations indicated that the lava dome was extruding at particularly high rates (up to 20m^3 was one estimate I heard), and at the peak of each cycle there was often ash venting. I proposed that the tremor bands were indicated of pressure cycles within the conduit - but caused by what? \n",
    "One suggestion is that the magma rises up the conduit in a stick-slip fashion. Basically, it gets stuck for a while, as the pressure builds below, and then shear fractures, allowing magma to suddenly extrude very quickly. \n",
    "\n",
    "Can we use some ObsPy STA/LTA detection tools to detect these tremor bands, in the same way we normally detect much shorter transient events, but just with longer STA/LTA settings? Let us try first on a single NSLC. This is based on examples at https://docs.obspy.org/tutorial/code_snippets/trigger_tutorial.html, except we use longer STA and LTA time windows (15 and 100 minutes respectively), and we add a despiking step which attempts to remove transient events lasting a minute or less from the data before running the STA/LTA:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72c792c7-70e8-4fca-ae80-442cac0172d0",
   "metadata": {},
   "source": [
    "### 4.3 Tremor band detection with ObsPy trigger methods\n",
    "\n",
    "#### 4.3.1 Single channel detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1a5863d-89cb-429d-a95e-e036dcb6f95f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from obspy.signal.trigger import plot_trigger, classic_sta_lta, recursive_sta_lta\n",
    "\n",
    "rsamObjSummer.despike(metrics='all')\n",
    "st = rsamObjSummer.to_stream()\n",
    "st.trim(obspy.core.UTCDateTime(1996,8,3), obspy.core.UTCDateTime(1996,8,5))\n",
    "\n",
    "sta_minutes = 15\n",
    "lta_minutes = 100\n",
    "threshON = 1.0\n",
    "threshOFF = 0.3\n",
    "\n",
    "cft = recursive_sta_lta(st[0].data, sta_minutes, lta_minutes)\n",
    "\n",
    "plot_trigger(st[0], cft, threshON, threshOFF)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c25c90a-5185-4386-8a03-05b9d5f04876",
   "metadata": {},
   "source": [
    "That seems to work quite well. Now let us try an event detector that uses several NSLC at once.\n",
    "\n",
    "#### 4.3.2 Multi-channel detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3426f231-2701-4d1f-83a3-eed89c8a35e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from obspy.signal.trigger import coincidence_trigger\n",
    "from pprint import pprint\n",
    "import numpy as np\n",
    "\n",
    "threshStations = 3\n",
    "\n",
    "trig = coincidence_trigger(\"recstalta\", threshON, threshOFF, st, threshStations, sta=sta_minutes*60, lta=lta_minutes*60, max_trigger_length=2*lta_minutes*60, delete_long_trigger=True)\n",
    "\n",
    "#pprint(trig)\n",
    "\n",
    "lendata = len(st[0].data)\n",
    "trdata = np.zeros( (lendata, ) )\n",
    "detectionTrace = obspy.Trace( data = trdata ) \n",
    "detectionTrace.id = 'XX.DETEC..TED'\n",
    "detectionTrace.stats.starttime = st[0].stats.starttime\n",
    "detectionTrace.stats.sampling_rate = st[0].stats.sampling_rate\n",
    "t = detectionTrace.times('utcdatetime')\n",
    "for thistrig in trig:\n",
    "    t0 = thistrig['time']\n",
    "    t1 = (thistrig['time'] + thistrig['duration'])\n",
    "    indices = np.where((t >= t0) & (t <= t1))\n",
    "    #print(t0, t1, indices)\n",
    "    detectionTrace.data[indices] = 1 #thistrig['duration']\n",
    "\n",
    "st3 = st.copy()\n",
    "st3.append(detectionTrace)\n",
    "st3.plot(equal_scale=False);\n",
    "\n",
    "detection_ON_times = [thistrig['time'].timestamp for thistrig in trig]\n",
    "detection_intervals_minutes = np.diff(np.array(detection_ON_times))/60\n",
    "for i,d in enumerate(detection_intervals_minutes):\n",
    "    print(f\"detection ON time for band {i}: {trig[i]['time']}, duration: {trig[i]['duration']/60} mins\")\n",
    "    print(f\"- interval (mins): {detection_intervals_minutes[i]}\")\n",
    "print(f\"detection ON time for band {i+1}: {trig[i+1]['time']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9499ef3a-527e-4675-b00c-ca6428c7bf54",
   "metadata": {},
   "source": [
    "The bottom trace here corresponds to the detected events, and you can see they line up pretty well with the tremor bands, except the first one was missed.\n",
    "\n",
    "This is similar to the banded tremor alarm system I wrote at MVO in 2000. And using this approach we can forecast the timing of the next tremor band. As it was the MVO Seismologist's job to manage the Operations Room, which included continuous seismic monitoring and two-way radio communications with MVO field crews, it was useful to predict tremor bands, as these were periods of heightened activity when field crews should not be on the flanks of the volcano.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "271b7072-bb6d-406b-8aea-6f38aef28077",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find peak value and peak time during each band\n",
    "import pandas as pd\n",
    "lod = []\n",
    "for thistrig in trig:\n",
    "    bandstarttime = thistrig['time']\n",
    "    bandendtime = thistrig['time'] + thistrig['duration']\n",
    "    bandTrace = maskedTrace.copy().trim(starttime=bandstarttime, endtime=bandendtime)\n",
    "    bandpeaktime = bandstarttime + bandTrace.data.argmax() * tr.stats.delta\n",
    "    band = {'starttime':bandstarttime, 'waxtime':bandpeaktime-bandstarttime, \\\n",
    "            'peaktime':bandpeaktime, 'wanetime':bandendtime-bandpeaktime, 'endtime':bandendtime, 'duration':thistrig['duration']}\n",
    "    lod.append(band)\n",
    "\n",
    "\n",
    "bandDf = pd.DataFrame(lod)\n",
    "print(bandDf)\n",
    "\n",
    "predicted = []\n",
    "for col in ['starttime', 'peaktime', 'endtime']:\n",
    "    interval = (bandDf.iloc[-1][col] - bandDf.iloc[0][col]) / (len(bandDf)-1) \n",
    "    predicted.append(bandDf.iloc[-1][col] + interval)\n",
    "print('\\nNext band prediction:')\n",
    "print(' - start: ',predicted[0])\n",
    "print(' - peak:  ',predicted[1])\n",
    "print(' - end:   ',predicted[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "552475ee-827c-4eef-bf30-cb70c41e3cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "st5 = rsamObjMidJuly.copy()\n",
    "st5.trim(starttime = obspy.core.UTCDateTime(1996,8,5,0,0,0), endtime = obspy.core.UTCDateTime(1996,8,5,3,0,0) )\n",
    "st5.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9accade-77e6-412d-a43f-53e54c614a46",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
